\documentclass{beamer}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{textcomp}
\usepackage{animate}
\usepackage{subcaption}
\usepackage[super]{nth}

\usetheme{Madrid}
\setbeamertemplate{caption}[numbered]

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thrm}{Theorem}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Agenda}
    \tableofcontents[currentsection]
  \end{frame}
}

\title[CZ4079 FYP Presentation] {CZ4079 Final Year Project}
\subtitle{A Machine Learning-Based Approach to \\Time-Dependent Shortest Path Queries}
\author[Wei Yumou]
{Wei Yumou}
\institute[]{School of Computer Science and Engineering \\ Nanyang Technological University}
\date[\today]{}
\logo{\includegraphics[height=1cm]{logo}}

\begin{document}
\frame{\titlepage}

\section{Introduction}

\begin{frame}
\frametitle{Introduction: Problem}
\begin{itemize}
	\item <2-> A \textbf{dynamic road network} $G=(V,E)$ with a time-dependent weight function $w : E,t \rightarrow \mathbb{R}$
	\item <3-> A \textbf{query} $Q(u,v,t)$ that asks for a shortest path from $u$ to $v$ departing at time moment $t$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction: General Approach}
\begin{itemize}
	\item <2-> Traditional \textbf{Bellman-Ford or Dijkstra's algorithm}	do not work with dynamic edge weights (``the curse of traditionality'')
	\item <3-> The new \textbf{machine learning-based approach} draws on collective wisdom of thousands of taxi drivers
	\item <4-> \textbf{Unsupervised learning} is employed to figure out the time-dependent edge costs
	\item <5-> A modified Dijkstra's algorithm calculates a shortest path on the fly
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction: Challenges}
\begin{itemize}
	\item <2-> Arbitrary $u$ and $v$
	\item <3-> Sparse sample points
	\item <4-> Limited GPS accuracy
\end{itemize}

\begin{figure}
\centering
\visible<3->{
\includegraphics[width=0.4\textwidth, height = 3.5cm]{low_sample_rate}
}
\visible<4->{
    \includegraphics[width=0.4\textwidth, height = 3.5cm]{limited_accuracy}
     \caption{Examples of challenges} 
}
\end{figure}
\end{frame}

\section{Preliminary Processing}
\begin{frame}
\frametitle{Preliminary Processing: Data Description}

\begin{itemize}
	\item Is collected from Computational Sensing Lab at Tsinghua University
	\item Contains 83 million GPS records from 8,602 taxis in Beijing during May of 2009
\end{itemize}

\begin{table}[h!]
\centering
\resizebox*{0.5\textwidth}{!}{
\begin{tabular}{ | l | l | l | }
\hline
\textbf{Field} & \textbf{Explanation} \\ \hline
CUID & ID for each taxi \\ \hline
UNIX\_EPOCH & Unix timestamp \\ \hline
GPS\_LONG & Longitude in WGS-84\\ \hline
GPS\_LAT & Latitude in WGS-84 \\ \hline
HEAD & Heading direction \\ \hline
SPEED & Instantaneous speed (m/s)\\ \hline
OCCUPIED & Hired (1) or not (0) \\ \hline
\end{tabular}
}
\caption{A summary of the seven original fields}\label{Ta:orig_field}
\end{table}
\end{frame}

\begin{frame}
\frametitle{Preliminary Processing: Reverse Geocoding}
\begin{itemize}
	\item <2-> GPS coordinate translation: 1.34\textdegree N, 103.68\textdegree E~\textrightarrow~SCSE, NTU
	\item <3-> China GPS shift problem: WGS84 v.s. BD09
	\item <4-> Solution: WGS84~$\xrightarrow{Baidu\ API}$~BD09~$\xrightarrow{Baidu\ API}$~Street
\end{itemize}
\visible<3->{
\begin{figure}[h!]
\includegraphics[scale=0.5]{gps_shift}
\centering
\caption{An example of China GPS shift problem}\label{Fig:gps_shift}
\end{figure}}
\end{frame}

\begin{frame}
\frametitle{Preliminary Processing: Outlier Detection}

\begin{figure}[h!]
\includegraphics[scale=0.58]{outlier}
\centering
\caption{An example of outliers}\label{Fig:outlier}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Preliminary Processing: Outlier Detection}

\begin{thrm}[\emph{Majority Clustering Theorem}]\label{Theorem: majority_clustering}
If a \textbf{reasonable reverse geocoder} is used to reverse-geocode a set of GPS data points which are mapped to a particular street \emph{in reality}, then, when plotted on a 2-D plane, majority (more than 50\%) of the points must be clustered together to form a rough shape that is similar to the shape of the street that they are supposed to be mapped to. 
\end{thrm}

\visible <2->{
Two-step procedure:

Outlier Detection = Outlier Identification + Outlier Removal
}

\end{frame}

\begin{frame}
\frametitle{Preliminary Processing: Outlier Detection}
\textbf{Outlier Identification}: Clustering
\begin{itemize}
	\item <2-> Sample point concentration \textrightarrow~cluster concentration
	\item <3-> Top $k\%$ ($k = 50$) largest clusters as groups of correct sample points
	\item <4-> 10 \texttimes~10 self-organising feature maps implementation
\end{itemize}

\visible <5->{
\textbf{Outlier Removal}: Distance Threshold $d_{max}$
}
\begin{itemize}
	\item <6-> Assign sample points to legal centroids no farther than $d_{max}$
	\item <7-> Remove all ``orphan'' sample points
	\item <8-> Use real physical distance on the Earth
	\item <9-> Set $d_{max}$ = 30m or 50m
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Preliminary Processing: Outlier Detection}

\begin{figure}[h!]
\includegraphics[scale = 0.48]{sofm_weights} 
\caption{A plot of neuron positions after training}
\label{Fig:sofm_weights}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Preliminary Processing: Outlier Detection}

\begin{figure}[h!]
\includegraphics[width = 0.5\linewidth]{bjtaxigps_30m} 
\includegraphics[width = 0.5\linewidth]{bjtaxigps_50m} 
\caption{A plot of sample points after outlier removal}\label{Fig:after_removal}
\end{figure}
\end{frame}
%
%\begin{frame}
%\frametitle{TrueSkill\texttrademark: Introduction}
%\begin{itemize}
%	\item A player-rating system developed by Microsoft Research to rank players on Xbox LIVE for matchmaking. 
%	\item A form of Bayesian Inference
%	\item Key idea to model each player's ``true skill'' as a Gaussian distribution $N\sim(s_{i} | \mu_{i}, \sigma_{i}^{2})$ (prior)
%	\item The outcome denoted as $r$ (evidence) $\rightarrow P(r | s_{i})$ (likelihood)
%\end{itemize}
%\begin{equation}
%P(s_{i} | r) = \frac{P(r | s_{i})P(s_{i})}{P(r)}
%\end{equation}
%\end{frame}
%
%\begin{frame}
%\frametitle{TrueSkill\texttrademark: How It Works}
%\begin{itemize}
%	\item Start with $N_{1}\sim(\mu_{1} = 25, \sigma_{1} = \frac{25}{3})$ and $N_{2}\sim(\mu_{2} = 25, \sigma_{2} = \frac{25}{3})$
%	\item As the tournament goes, TrueSkill\texttrademark~updates each player's skill based on match outcomes ($Y = 1$ when Player 1 win)
%	\item The wining probability of \textbf{an upcoming match} can be calculated based on each player's true skill distribution
%\begin{equation}
%P(Y = 1) = \Phi(\frac{\mu_{1} - \mu_{2}}{\sqrt{2\beta^{2} + \sigma_{1}^{2} + \sigma_{2}^{2}}})
%\end{equation}
%where $\Phi$ is the standard Gaussian CDF and $\beta$ represents performance variations around skill (default value $\sigma/2$)
%\end{itemize}
%\end{frame}
%
%%\begin{frame}
%%\frametitle{TrueSkill\texttrademark: Demo}
%%\animategraphics[loop,controls,width=0.7\linewidth]{2}{images/trueskill_demo/}{0}{36}
%%\end{frame}
%
%\begin{frame}
%\frametitle{Models}
%\begin{itemize}
%	\item{MOV = Margin of Victory}
%	\item{HCA = Home Court Advantage}
%	\item{OAD = Overtime as Draw}
%\end{itemize}
%\begin{table}[h!]
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{ | l | l | l |}
%\hline
%\textbf{Model} & \textbf{Description} & \textbf{Parameters} \\ \hline
%Na\"{i}ve TrueSkill & Bare TrueSkill\texttrademark & $\mu = 25, \sigma = 25/3$ \\ \hline
%MOV Unfolding & If MOV $>m$, recursively treat it as multiple victories & $m$ = 11\\ \hline
%HCA & Deduct $h$ scores if victory at home court & $h$ = 1.5 \\ \hline
%OAD & Treat overtime as having equal scores & -\\ \hline
%\end{tabular}}
%\caption{Models}\label{Ta:model}
%\end{table}
%\end{frame}
%
%\begin{frame}
%\frametitle{Evaluation: Log loss}
%\begin{equation}
%LogLoss = -\frac{1}{n}\sum_{i=1}^{n}[y_{i}\ln(\hat{y_{i}}) + (1 - y_{i})\ln(1 - \hat{y_{i}})]
%\end{equation}
%where
%\begin{itemize}
%	\item $n$ is the number of games played 
%	\item $\hat{y_{i}}$ is the predicted probability of team 1 beating team 2
%	\item $y_{i}$ is 1 if team 1 wins, 0 if team 2 wins
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Fine-tuning}
%\begin{itemize}
%	\item Idea: if we are very confident about a prediction, why not ``all-in''?
%	\item A tradeoff between a small reward with large probability and a huge penalty with (hopefully) small probability
%	\item How high is the penalty? For example, we turn a 0.9 prediction into 1
%		\begin{itemize}
%			\item If the outcome is 1, the score will improve by $\frac{-\ln(0.9)}{2279} = 4.623\times 10^{-5}$
%			\item However if the outcome is 0, the penalty will be infinity
%		\end{itemize}
%	\item Our adjustments: make all predictions $>0.9$ be 1 and all predictions $<0.1$ be 0
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Results}
%\begin{itemize}
%	\item{MOV = Margin of Victory}
%	\item HCA $=$ Home Court Advantage
%	\item OAD $=$ Overtime as Draw
%\end{itemize}
%\begin{table}[h!]
%\centering
%\begin{tabular}{ | l | l | l | l |}
%\hline
%\textbf{Model} & \textbf{Log loss} & \textbf{Rank} & \textbf{Top} \\ \hline
%Logistic Regression & 0.588910 & \nth{283} & 47.08\% \\ \hline
%Na\"{i}ve TrueSkill & 0.527775 & \nth{11} & 1.67\% \\ \hline
%MOV Unfolding & 0.527510 & \nth{11} & 1.67\% \\ \hline
%HCA & 0.524036 & \nth{6} & 0.83\%\\ \hline
%OAD & 0.520875 & \nth{6} & 0.83\% \\ \hline
%Fine-tuning  & 0.513572 & \nth{5} & 0.67\%\\ \hline
%%Fine-tuning  & 0.472260 & \nth{1} & 0.00\%\\ \hline
%\end{tabular}
%\caption{Final Results}\label{Ta:results}
%\end{table}
%\end{frame}
%
%\begin{frame}
%\frametitle{Conclusion}
%\begin{itemize}
%	\item TrueSkill\texttrademark~algorithm can give a reasonable estimate of the ``true skill'' of a player (team) and calculate the wining probability of a future match
%	\item We selected three more features to give a better prediction
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Questions \& Answers}
%\begin{center}
%\huge Any questions?
%\end{center}
%\end{frame}

\end{document}